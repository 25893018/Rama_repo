
https://komodor.com/learn/how-to-fix-createcontainerconfigerror-and-createcontainer-errors/


ConfigMap is missing—a ConfigMap stores configuration data as key-value pairs.	Identify the missing ConfigMap and create it in the namespace, 
or mount another, existing ConfigMap.
Secret is missing—a Secret is used to store sensitive information such as credentials.	Identify the missing Secret and create it in the namespace,
or mount another, existing Secret.

commands:
kubectl get pods
kubectl describe pod [name] 
kubectl describe pod [name] /tmp/troubleshooting_describe_pod.txt
kubectl get configmap configmap name
kubectl get secret
kubectl get nodes --show-labels
kubectl cluster-info 
kubectl get nodes

kubectl create -f busybox.yaml --> this is not recommned way to create pods becz when pod is down/deleted , it will not bring up automatically when using this
command. insted always use below run command.
kubectl run nginx --nginxname=nginx   --> this run command will work like a deployment ..recommed to use this command to bring up /pod monitor continusely if 
or
kubectl run -f  busybox.yaml

unfortunatly delete pod .. it will create new pod automatically.

kubectl get all 
kubectl delete pod pod name
kubectl get deployment
kubectl delete deployment/nginx
note when deployment/nginx image is deleted, automatically releavent pod also will delete.

kubectl api -versions  ==>to check api versions
yaml:==Aint markup language 
yaml is used for to define resources in k8's
yaml file you can keep in github and use.

depolyments : 
================Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features(
monitor pod always). Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration 
or don't require updates at all. deployment is nothing but a suppose if pod is mistakenly deleted ,it will brimg up pod automatically.

replicat set is nothing but a you can create number of pods 
=========
example: replicat set =3 means it will create 3 pods at a time with that mentioned image.
*****deployment have two types of methods to create/update versions:
recreate --> in this method we have down time to recreate pods. you need to bring down the pods to bring up another version pods. so there is a down time here.
========
rollingout  --> in this method it will not down all pods at a time insted first pod will be down and then version will be update.
=========
again second pod will be down and then version will be update
again third pod will be down and then version will be update.......;

default will be in k8's rollingout method apply.
kubectl create -f nginxdeploy.yml   --> this is initial creation 
kubectl get deployment.apps/nginxdeploy

kubectl apply -f nginxdeploy.yml    --> this command is for when you are updating version changes/updates for continusly to scalup or down pods
kubectl delete -f nginxdeploy.yml
kubectl get deployment.apps/nginxdeploy
kubectl get pods

kubectl set image deployment/nginix nginx=nginx:latest  --> to move alphine version to latest version 

kubectl get deployment
kebectl get all
you can see now difference old version to new version pods are updated.

suppose if i want to move previous version use below rollout command 

kubectl rollout undo deployment//nginx   --> to move old /previous version
kubectl get replicaset

supoose if you want to increse replicas use below command: increase/scaleup replicas in yaml file and then use apply command to update
kubectl apply -f nginx.yaml
to see error details for pod
=====================
kubectl describe pod podname

please note always try to change yaml file for image updates. that is the best option insted running commands.
kubectl api-versions
kubectl explain deployment    --> to show deployment version to use in yaml file
kubectl rollout status
kubectl rollout history deployment/nginx
kubectl rollout status deployment/nginx
kubectl get resources -o yaml > myfile.yaml  --> to  get another resource info

kubectl api-resources
kubectl api-versions
kubectl pod.spec
kubectl pod.volume
kubectl get pods -o wide

To login into container: kubectl exec -it container -c busybox1 /bin/bash
ip a

sample yaml file for deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: web
spec:
  selector:
    matchLabels:
      app: web
  replicas: 5
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
       —name: nginx
          image: nginx:alphine
          ports:
           —containerPort: 80
 ==========================================================================================
docker ps --> it will display container details 
docker ps -a --> it will show more details  
docker run centos echo hello --> it will pull and download the image and pring message hello
docker search centos --? to search any image using this command

NODE TYPE COMPONENT WHERE TO FIND LOGS
Master API Server	 /var/log/kube-apiserver.log
Master Scheduler /var/log/kube-scheduler.log
Master Controller Manager	 /var/log/kube-controller-manager.log

Worker Kubelet /var/log/kubelet.log
Worker Kube Proxy	 /var/log/kube-proxy.log

1. What are the features of Kubernetes?
==================================
The features of Kubernetes, are as follows:

automated schduling:- k8's provides advanced schdular to launch container on cluster nodes.
self healing capabilites:- reschduling, replacing and restarting the containers which are died.
Automated rollouts and rollback:- k8's support rollout and rollback for the desigerd state of the containerezed application.
horzental scaling and load balancing :- k8's can scale up and scale down the application based on requirements.

2. How is Kubernetes different from Docker Swarm?

Docker Swarm can’t do auto-scaling (as can Kubernetes); 
Docker Swarm doesn’t have a GUI; Kubernetes has a GUI in the form of a dashboard
Docker Swarm does automatic load balancing of traffic between containers in a cluster, while Kubernetes requires manual intervention for load balancing such traffic.
Docker requires third-party tools like ELK stack for logging and monitoring, while Kubernetes has integrated tools for the same
Docker Swarm can share storage volumes with any container easily, while Kubernetes can only share storage volumes with containers in the same pod
Docker can deploy rolling updates but can’t deploy automatic rollbacks; Kubernetes can deploy rolling updates as well as automatic rollbacks

3. 

What is a Kubernetes Ingress Controller?
A Kubernetes Ingress Controller is a software component responsible for implementing the rules specified in the Ingress resource. It acts as a gateway between external traffic and services running within a Kubernetes cluster, enabling seamless routing and service exposure.

Why do I need an Ingress Controller?
An Ingress Controller simplifies the process of managing external traffic and service exposure in Kubernetes. It provides advanced routing capabilities, load balancing, SSL termination, and security features, making it easier to expose services to the outside world while ensuring scalability and reliability.

What are some popular Ingress Controllers?
Some widely used Ingress Controllers in the Kubernetes ecosystem include Nginx Ingress Controller, Traefik, and HAProxy Ingress Controller. These controllers offer various features and integrations, allowing you to choose the one that best fits your requirements.

What benefits do Ingress Controllers offer?
Ingress Controllers provide several benefits, including simplified service exposure, advanced traffic management, load balancing, enhanced security through SSL termination and authentication mechanisms, scalability, and flexibility to adapt to changes in the Kubernetes cluster.

Can I use multiple Ingress Controllers in a Kubernetes cluster?
While it is technically possible to use multiple Ingress Controllers in a Kubernetes cluster, it is generally recommended to choose one as the primary controller to avoid conflicts and ensure consistent routing rules and behavior.

Are Ingress Controllers compatible with cloud-based load balancers?
Yes, Ingress Controllers can work with cloud-based load balancers. In such cases, the Ingress Controller acts as an interface between Kubernetes and the cloud load balancer, providing additional routing and management capabilities specific to Kubernetes services.


Create a new Kubernetes namespace
=======================================
$ kubectl create namespace <name>
A YAML file, like any other Kubernetes resource, can be created and used to create a namespace:

my-namespace.yaml: 

apiVersion: v1
kind: Namespace
metadata:
  name: <insert-namespace-name-here>
Then run:

$ kubectl create -f my-namespace.yaml
The command to display all namespaces in the cluster is:
$ kubectl get namespace
How to switch between Kubernetes namespaces?
When addressing namespaces, actions must include the “-namepsace=” option in the command. Because this can be time-consuming, the default namespace can be modified by using the kubectl config command to set the namespace in the cluster context.

To switch from the default namespace to ‘K21,’ 
for example, type: 
command:- kubectl config set-context –current –namespace=K21. This will make ‘K21’ the default namespace for all future kubectl commands.

How to rename a Kubernetes namespace?
Renaming a Kubernetes namespace is not recommended, so choose namespaces (other than the default) with caution.

How to delete a Kubernetes namespace?
Delete a namespace with the below command:
$ kubectl delete namespaces <name>

Because deletion is an asynchronous activity, the namespace will appear as ‘terminating’ until it is deleted.

Warning⚠️:  The deletion of a namespace is the final act. Everything in the namespace will be deleted, including all services, running pods, and artifacts.
Garbage collection will be performed on anything that previously existed in that namespace. Before performing this action, ensure that everything in the namespace has been deleted.

Namespace:
=========
Kubernetes uses namespaces to organize objects/resources in the cluster. You can think of each namespace as a folder that holds a set of objects. 
By default, the kubectl command-line tool interacts with the default namespace. If you want to use a different namespace, 
you can pass kubectl the --namespace flag. For example, kubectl --namespace=mystuff references objects in the mystuff namespace.

The kubectl command-line utility is a powerful tool.

kubectl run namespace citi  --> it will create namespace called citi
or
kubectl deploy namespace citi
kubectl get all --all-namespaces   to see all namespaces
kubectl get namespaces
kubectl get pods -n trietree   --> here n attribute will go and check in that trietree namespace how many pods are there and those will display.
kubectl get all -n trietree   ==> to show all info in that namespace trietree
kubectl get ns --> ns is short form of namespace

to switch default namespace to specific namespace
kubectl config set-context --current --namespace=default
kubectl config set-context --current --namespace=trietree
kubect get pods
kubectl get pods -o wide   --> it will show ip addresss as well

=========================
networking:
===================
default ip will be 172.17.0.1 when pod is created it will be automatically allowcated
pods will create/manage using deployments(deployment.yaml)
services will manage deployments
using labels services will connect to deployments/pods. if you want to connect particaluar application, you can mention as lable name in yaml file, 
then when do deployment automatically services will connect to app/pods.

kubectl get pods -o wide   --> it will show ip addresss as well
kubectl run/apply -f deploy.yaml
kubectl describe pod twocontainers  --> two containes is a pod name where we mentioned in yaml file under metadata
to see more details use describe and use created two containers in pod using same ip

kubectl exec -it twocontainers -c busybox1 /bin/sh   --> here busybox1 is a container name and twocontainers is a pod name. we are checking here busybox1 container
ip 
kubectl exec -it twocontainers -c busybox2 /bin/sh  -->  here busybox2 is a container name and twocontainers is a pod name. we are checking here busybox2 container ip 

ttiwill be same ip for both containers, so in pod if you have multiple containers, ip will be same for all containers.

services:= suppose i have two nodes , one node have two pods and another node have one pod.each pod allowcated one ip for each pod, so total 3 ips allowcated 
for pods. every node have one kubeproxey and iptable( in iptable store all ip addresses) , hence kubeproxey will check ipaddresses in iptables , based on ip 
services will connect pods using labels , it will work like load balancer.

using labels we can create service object 
how service interact with pods:- 
===========================
every node have one kubeproxey and iptable.
kubeproxey will route ip traffic to the IPtables

node --> kubeproxey-->iptables -->pod -->services(lables)

so here using labels services will connect to pods based on ip
in service object we have ipaddress, target port and endpoint.

service types: 
=============
nodeport  --range start from 30000 to 32767
clusterip
load balancer.


kubectl get nodes -o wide
ping ip   --> here yu can ping node ip 
kubectl get pods -o wide
ping ip   --> but here you cannot ping pod ip

sample yaml for pod
==================
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
containers:
- name: nginx-container
image: nginx

sample yaml file for service.yaml:
==============================
apiVersion: v1
kind: Service
metadata:
  name: app-service
  spec:
selector:
    app: myapp
ports:
- targetPort: 80
port: 80
nodePort: 30020
type: nodePort

kindly note when you are creating service lable and selector app names are should be same . then service will connect to application pod

kubectl run -f busyboxlable.yaml
kubectl run -f service.yaml
kubectl get svc or service
kubectl get nodes -o wide
curl http://nodeip:nodeportnumber   --> you can access /expose application using service and also you can browse.

clusterIP:--
=======
suppose i have three pods , each pod have one ip, when pod is down and created new pod, then it will create new ip and ip will change.
samething will happen for other pods as well, to avoid this you can directly connect to cluster ip and can use for all PODS, just like a interface.

Load Balancer:
============            you can assing ip pubicly/externally to expose service/application.

kubectl get svc or service

when run this command you can see external-ip colum have this load balancer ip's


Storage configuration:(volumes)
====================

kubectl exec -it vol2 touch /test/testfile -- to create test file  
kubectl exec -it vol2 -- ls -l /test/testfile

(-- ls -l)  --> here if you have -- that means kubectl knows this is a linux command to display output, otherwise it will give error
not here any where if you are using linux command you can use -- using kubectl.

shared volume/file --: same file it will create for multiple pods. it means sharing disk for multiple containers:


apiVersion: v1
kind: Pod
metadata:
  name: morevolume2
  spec:
  containers:
 - name: centos1
 image: centos:7
 command: 
 - sleep 10
volumeMounts:
    - mountPath: /centos
      name: test
  - name: centos2
 image: centos:7
 command: 
 - sleep 10
volumeMounts:
    - mountPath: /centos2
      name: test
   volumes:
    name: test
    emptyDir: ()

test is a disk name, it will share same disk two different containers

kubectl exec  -it morevolume2 -c centos1 -- touch /centos/testfile  --> we have created file in centos1 ,
but it will show in centos2 as well becz volume created on shared.

kubectl exec  -it morevolume2 -c centos2 -- ls -l  /centos2  --> to see file on centos2 whether testfile created or not

persistent volume:
===============

kubectl get pv (persistent volume)
kubectl get pvc (persistent volume claim)

stateful sets:---
==============
normallyy in deployment pods will create all at a time when we create. if any time pod is down and up again IP/hostname will change ,that will be the problem
in production. to avoid this we can use stateful sets. when you use statfulsets , it will create pods one by one and create separate id for each pod. hence
it will not change ip/hostname using stateful sets.

$ kubectl delete statefulsets zk   -> zk is statefulset name giveen in yaml file.

headless service:
===============
******when some one asking to create headless service , then clusterip should be "None".
also in headless service we dont assign IPaddress, insted it will point to always Pod 0
it will create DNS and write only to master(Pod 0)
remining all other pods internally it will communicate to service and read data.

Troubleshooting pods:
==================
kubectl exec -it podname -- /bin/sh   --> to enter into pod application

when you do trouble shooting ip a command will not work inside the pod container, becz container has only application dependencies and libraries.

so to troubleshoot more goto /proc
do ls 
then you can go to any process like ---- cd 1, or cd 100 or cat processname , if you want to comeout just do exit.
kubectl logs podname --> to check logs 
kubectl describe pod podname  --> to see more details for events/errors.
kubectl cordon -h ==> cordon used for unschdule the node
kubectl drain -h --> drain used for to move node into maintense status
kubectl uncordon -h ==> uncordon used for to move the node normal status/schudle.


kubernetes API management: API is nothing but what ever resoucres are making it to avilaible for end users , that is called API
================
using kubeproxey or curl we can access api's where using in yaml files.
 yaml file --> aPI -->api server (will provide all apis and using kubeproxey we can access all apis)
if you want to access api's you need authorized certificates like public, private, CA authorized certificates(mostly via TLS security). transfort layer security

 normally in k8's directly using kubeproxey or curl we can access api's where using in yaml files insted of certications.
to fetch details from API server:
******to expose port/service
kubectl proxy --port=8001
curl http://localhost:8001/version
curl http://localhost:8001/api/v1/namespaces/default
curl http://localhost:8001/api/v1/namespaces/default/pods
curl http://localhost:8001/api/v1/namespaces/default/pods/busybox1   --> here busybox1 is a container name to see details.
curl -XDELETE http://localhost:8001/api/v1/namespaces/default/pods/busybox1   --> it will delete busybox1 pod try it on Linux mechine.
kubectl api-versions --> to see all apis
kubectl api-resources --> to see all resources

kubectl explain pod --> it will give which api version to use in yaml file ..very importent to know.
kubectl explain statefulsets

Ingress: to expose traffice using http & https .

Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster.
Traffic routing is controlled by rules defined on the Ingress resource.
==========
***you can expose services using clusterip , nodeport, load balancer and ingress.
using ingress, you can redirect services from one page to another page easily
example:- i have one application called api.com , in that i have publisher page, testing page, developer page....using same server(api.com) i want to redirect 
all pages one by one or services , we can use for ingress.

using nodeport you cannot expose service/applications publicly, this is within the application
if you want to expose publicly then you should go for load balancer or ingress.

through ingress controller traffaic will route to services and also using same ingress you can expose number of services.

there are two types of ingress:1.  ingress - nginx 2. kubernetes ingress.
=======================
minikube addons list
if you want to enable ingress in minikube then use below command
minikube addons enable ingress
using ingress mandatory yaml file you can create ingress custom controller in cloud --> search in google ingress mandatory yaml
by default ingress controller is there in cloud.

minikube ssh --> it means going inside minikube
docker ps | grep -i ingress   --> to see ingress controller running or not

kubectl get deployments
minikube ip








