
https://komodor.com/learn/how-to-fix-createcontainerconfigerror-and-createcontainer-errors/


ConfigMap is missing—a ConfigMap stores configuration data as key-value pairs.	Identify the missing ConfigMap and create it in the namespace, 
or mount another, existing ConfigMap.
Secret is missing—a Secret is used to store sensitive information such as credentials.	Identify the missing Secret and create it in the namespace,
or mount another, existing Secret.

commands:
kubectl get pods
kubectl describe pod [name] 
kubectl describe pod [name] /tmp/troubleshooting_describe_pod.txt
kubectl get configmap configmap name
kubectl get secret
kubectl get nodes --show-labels
kubectl cluster-info 
kubectl get nodes

kubectl create -f busybox.yaml --> this is not recommned way to create pods becz when pod is down/deleted , it will not bring up automatically when using this
command. insted always use below run command.
kubectl run nginx --nginxname=nginx   --> this run command will work like a deployment ..recommed to use this command to bring up /pod monitor continusely if 
or
kubectl run -f  busybox.yaml

unfortunatly delete pod .. it will create new pod automatically.

kubectl get all 
kubectl delete pod pod name
kubectl get deployments

kubectl api -versions  ==>to check api versions
yaml:==Aint markup language 
yaml is used for to define resources in k8's
yaml file you can keep in github and use.

depolyments : deployment is nothing but a suppose if pod is mistakenly deleted ,it will brimg up pod automatically.
========
replicat set is nothing but a you can create number of pods 
=========
example: replicat set =3 means it will create 3 pods at a time with that mentioned image.
*****deployment have two types of methods to create/update versions:
recreate --> in this method we have down time to recreate pods. you need to bring down the pods to bring up another version pods. so there is a down time here.
========
rollingout  --> in this method it will not down all pods at a time insted first pod will be down and then version will be update.
=========
again second pod will be down and then version will be update
again third pod will be down and then version will be update.......;

default will be in k8's rollingout method apply.
kubectl create -f nginxdeploy.yml   --> this is initial creation 
kubectl get deployment.apps/nginxdeploy

kubectl apply -f nginxdeploy.yml    --> this command is for when you are updating version changes/updates for continusly to scalup or down pods
kubectl delete -f nginxdeploy.yml
kubectl get deployment.apps/nginxdeploy
kubectl get pods

kubectl set image deployment/nginix nginx=nginx:latest  --> to move alphine version to latest version 

kubectl get deployment
kebectl get all

you can see now difference old version to new version pods are updated.

suppose if i want to move previous version use below rollout command 

kubectl rollout undo deployment//nginx   --> to move old /previous version
kubectl get replicaset

to see error details for pod
=====================
kubectl describe pod podname

please note always try to change yaml file for image updates. that is the best option insted running commands.
kubectl api-versions
kubectl explain deployment    --> to show deployment version to use in yaml file
kubectl rollout status
kubectl rollout history deployment/nginx
kubectl rollout status deployment/nginx
kubectl get resources -o yaml > myfile.yaml  --> to  get another resource info

kubectl api-resources
kubectl api-versions
kubectl pod.spec
kubectl pod.volume
kubectl get pods -o wide

To login into container: kubectl exec -it container -c busybox1 /bin/bash
ip a

sample yaml file for deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: web
spec:
  selector:
    matchLabels:
      app: web
  replicas: 5
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
       —name: nginx
          image: nginx:alphine
          ports:
           —containerPort: 80
 ==========================================================================================
docker ps --> it will display container details 
docker ps -a --> it will show more details  
docker run centos echo hello --> it will pull and download the image and pring message hello
docker search centos --? to search any image using this command

NODE TYPE COMPONENT WHERE TO FIND LOGS
Master API Server	 /var/log/kube-apiserver.log
Master Scheduler /var/log/kube-scheduler.log
Master Controller Manager	 /var/log/kube-controller-manager.log

Worker Kubelet /var/log/kubelet.log
Worker Kube Proxy	 /var/log/kube-proxy.log

1. What are the features of Kubernetes?
==================================
The features of Kubernetes, are as follows:

automated schduling:- k8's provides advanced schdular to launch container on cluster nodes.
self healing capabilites:- reschduling, replacing and restarting the containers which are died.
Automated rollouts and rollback:- k8's support rollout and rollback for the desigerd state of the containerezed application.
horzental scaling and load balancing :- k8's can scale up and scale down the application based on requirements.

2. How is Kubernetes different from Docker Swarm?

Docker Swarm can’t do auto-scaling (as can Kubernetes); 
Docker Swarm doesn’t have a GUI; Kubernetes has a GUI in the form of a dashboard
Docker Swarm does automatic load balancing of traffic between containers in a cluster, while Kubernetes requires manual intervention for load balancing such traffic.
Docker requires third-party tools like ELK stack for logging and monitoring, while Kubernetes has integrated tools for the same
Docker Swarm can share storage volumes with any container easily, while Kubernetes can only share storage volumes with containers in the same pod
Docker can deploy rolling updates but can’t deploy automatic rollbacks; Kubernetes can deploy rolling updates as well as automatic rollbacks

3. 

What is a Kubernetes Ingress Controller?
A Kubernetes Ingress Controller is a software component responsible for implementing the rules specified in the Ingress resource. It acts as a gateway between external traffic and services running within a Kubernetes cluster, enabling seamless routing and service exposure.

Why do I need an Ingress Controller?
An Ingress Controller simplifies the process of managing external traffic and service exposure in Kubernetes. It provides advanced routing capabilities, load balancing, SSL termination, and security features, making it easier to expose services to the outside world while ensuring scalability and reliability.

What are some popular Ingress Controllers?
Some widely used Ingress Controllers in the Kubernetes ecosystem include Nginx Ingress Controller, Traefik, and HAProxy Ingress Controller. These controllers offer various features and integrations, allowing you to choose the one that best fits your requirements.

What benefits do Ingress Controllers offer?
Ingress Controllers provide several benefits, including simplified service exposure, advanced traffic management, load balancing, enhanced security through SSL termination and authentication mechanisms, scalability, and flexibility to adapt to changes in the Kubernetes cluster.

Can I use multiple Ingress Controllers in a Kubernetes cluster?
While it is technically possible to use multiple Ingress Controllers in a Kubernetes cluster, it is generally recommended to choose one as the primary controller to avoid conflicts and ensure consistent routing rules and behavior.

Are Ingress Controllers compatible with cloud-based load balancers?
Yes, Ingress Controllers can work with cloud-based load balancers. In such cases, the Ingress Controller acts as an interface between Kubernetes and the cloud load balancer, providing additional routing and management capabilities specific to Kubernetes services.


Create a new Kubernetes namespace
=======================================
$ kubectl create namespace <name>
A YAML file, like any other Kubernetes resource, can be created and used to create a namespace:

my-namespace.yaml: 

apiVersion: v1
kind: Namespace
metadata:
  name: <insert-namespace-name-here>
Then run:

$ kubectl create -f my-namespace.yaml
The command to display all namespaces in the cluster is:
$ kubectl get namespace
How to switch between Kubernetes namespaces?
When addressing namespaces, actions must include the “-namepsace=” option in the command. Because this can be time-consuming, the default namespace can be modified by using the kubectl config command to set the namespace in the cluster context.

To switch from the default namespace to ‘K21,’ 
for example, type: 
command:- kubectl config set-context –current –namespace=K21. This will make ‘K21’ the default namespace for all future kubectl commands.

How to rename a Kubernetes namespace?
Renaming a Kubernetes namespace is not recommended, so choose namespaces (other than the default) with caution.

How to delete a Kubernetes namespace?
Delete a namespace with the below command:
$ kubectl delete namespaces <name>

Because deletion is an asynchronous activity, the namespace will appear as ‘terminating’ until it is deleted.

Warning⚠️:  The deletion of a namespace is the final act. Everything in the namespace will be deleted, including all services, running pods, and artifacts.
Garbage collection will be performed on anything that previously existed in that namespace. Before performing this action, ensure that everything in the namespace has been deleted.

Namespace:
=========
Kubernetes uses namespaces to organize objects/resources in the cluster. You can think of each namespace as a folder that holds a set of objects. 
By default, the kubectl command-line tool interacts with the default namespace. If you want to use a different namespace, 
you can pass kubectl the --namespace flag. For example, kubectl --namespace=mystuff references objects in the mystuff namespace.

The kubectl command-line utility is a powerful tool.

kubectl run namespace citi  --> it will create namespace called citi
or
kubectl deploy namespace citi
kubectl get all --all-namespaces   to see all namespaces
kubectl get namespaces
kubectl get pods -n trietree   --> here n attribute will go and check in that trietree namespace how many pods are there and those will display.
kubectl get all -n trietree   ==> to show all info in that namespace trietree
kubectl get ns --> ns is short form of namespace

to switch default namespace to specific namespace
kubectl config set-context --current --namespace=default
kubectl config set-context --current --namespace=trietree
kubect get pods
kubectl get pods -o wide   --> it will show ip addresss as well

=========================
networking:
===================
default ip will be 172.17.0.1 when pod is created it will be automatically allowcated
pods will create/manage using deployments(deployment.yaml)
services will manage deployments
using labels services will connect to deployments/pods. if you want to connect particaluar application, you can mention as lable name in yaml file, 
then when do deployment automatically services will connect to app/pods.

kubectl get pods -o wide   --> it will show ip addresss as well
kubectl run/apply -f deploy.yaml
kubectl describe pod twocontainers  --> two containes is a pod name where we mentioned in yaml file under metadata
to see more details use describe and use created two containers in pod using same ip

kubectl exec -it twocontainers -c busybox1 /bin/sh   --> here busybox1 is a container name and twocontainers is a pod name. we are checking here busybox1 container ip 
kubectl exec -it twocontainers -c busybox2 /bin/sh  -->  here busybox2 is a container name and twocontainers is a pod name. we are checking here busybox2 container ip 

ttiwill be same ip for both containers, so in pod if you have multiple containers, ip will be same for all containers.

services:= suppose i have two nodes , one node have two pods and another node have one pod.each pod allowcated one ip for each pod, so total 3 ips allowcated 
for pods. every node have one kubeproxey and iptable( in iptable store all ip addresses) , hence kubeproxey will check ipaddresses in iptables , based on ip 
services will connect pods using labels , it will work like load balancer.

using labels we can create service object 
how service interact with pods:- 
===========================
every node have one kubeproxey and iptable.
kubeproxey will route ip traffic to the IPtables

node --> kubeproxey-->iptables -->pod -->services(lables)

so here using labels services will connect to pods based on ip
in service object we have ipaddress, target port and endpoint.

service types: 
=============
nodeport  --range start from 30000 to 32767
clusterip
load balancer.


kubectl get nodes -o wide
ping ip   --> here yu can ping node ip 
kubectl get pods -o wide
ping ip   --> but here you cannot ping pod ip

sample yaml for pod
==================
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
containers:
- name: nginx-container
image: nginx

sample yaml file for service.yaml:
==============================
apiVersion: v1
kind: Service
metadata:
  name: app-service
  spec:
selector:
    app: myapp
ports:
- targetPort: 80
port: 80
nodePort: 30020
type: nodePort

kindly note when you are creating service lable and selector app names are should be same . then service will connect to application pod

kubectl run -f busyboxlable.yaml
kubectl run -f service.yaml
kubectl get svc or service
kubectl get nodes -o wide
curl http://nodeip:nodeportnumber   --> you can access /expose application using service and also you can browse.

clusterIP:--
=======
suppose i have three pods , each pod have one ip, when pod is down and created new pod, then it will create new ip and ip will change.
samething will happen for other pods as well, to avoid this you can directly connect to cluster ip and can use for all PODS, just like a interface.

Load Balancer:
============            you can assing ip pubicly/externally to expose service/application.

kubectl get svc or service

when run this command you can see external-ip colum have this load balancer ip's


Storage configuration:(volumes)
====================

kubectl exec -it vol2 touch /test/testfile -- to create test file  
kubectl exec -it vol2 -- ls -l /test/testfile

shared volume/file --: same file it will create for multiple pods. it means sharing disk for multiple containers:


apiVersion: v1
kind: Pod
metadata:
  name: morevolume2
  spec:
  containers:
 - name: centos1
 image: centos:7
 command: 
 - sleep 10
volumeMounts:
    - mountPath: /centos
      name: test
  - name: centos2
 image: centos:7
 command: 
 - sleep 10
volumeMounts:
    - mountPath: /centos2
      name: test
   volumes:
    name: test
    emptyDir: ()

test is a disk name, it will share same disk two different containers

kubectl exec  -it morevolume2 -c centos1 -- touch /centos/testfile  --> we have created file in centos1 ,
but it will show in centos2 as well becz volume created on shared.

kubectl exec  -it morevolume2 -c centos2 -- ls -l  /centos2  --> to see file on centos2 whether testfile created or not

persistent volume:
===============

kubectl get pv (persistent volume)
kubectl get pvc (persistent volume claim)

Ingress: to expose http & https traffic.
=====




